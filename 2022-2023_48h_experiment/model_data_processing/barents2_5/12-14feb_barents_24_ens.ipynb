{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47d781ca-de57-4d38-832d-6626b8d1f99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For model data\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#for saving output\n",
    "import csv\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "#for calculating time difference\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "\n",
    "#Tools for evaraging model data\n",
    "sys.path.append(r'../tools')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10a9989c-f942-4be0-ba6b-8a7280aab16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "syntax error, unexpected $end, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\n",
      "context: ^\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno -72] NetCDF: Malformed or inaccessible DAP2 DDS or DAP4 DMR response: 'https://thredds.met.no/thredds/dodsC/fou-hi/barents_eps_eps/barents_eps_20230211T00Z.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xarray/backends/file_manager.py:211\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xarray/backends/lru_cache.py:56\u001b[0m, in \u001b[0;36mLRUCache.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m---> 56\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mmove_to_end(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: [<class 'netCDF4._netCDF4.Dataset'>, ('https://thredds.met.no/thredds/dodsC/fou-hi/barents_eps_eps/barents_eps_20230211T00Z.nc',), 'r', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False)), '9efa74b8-61a6-42d5-a994-6e9cf5a3e8a8']",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset0 \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://thredds.met.no/thredds/dodsC/fou-hi/barents_eps_eps/barents_eps_20230211T00Z.nc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m dataset1 \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://thredds.met.no/thredds/dodsC/fou-hi/barents_eps_eps/barents_eps_20230211T06Z.nc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m dataset2 \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://thredds.met.no/thredds/dodsC/fou-hi/barents_eps_eps/barents_eps_20230211T12Z.nc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xarray/backends/api.py:570\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    558\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    559\u001b[0m     decode_cf,\n\u001b[1;32m    560\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    566\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[1;32m    567\u001b[0m )\n\u001b[1;32m    569\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 570\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    577\u001b[0m     backend_ds,\n\u001b[1;32m    578\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    589\u001b[0m )\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:602\u001b[0m, in \u001b[0;36mNetCDF4BackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, lock, autoclose)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_dataset\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]  # allow LSP violation, not supporting **kwargs\u001b[39;00m\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    583\u001b[0m     filename_or_obj: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m os\u001b[38;5;241m.\u001b[39mPathLike[Any] \u001b[38;5;241m|\u001b[39m BufferedIOBase \u001b[38;5;241m|\u001b[39m AbstractDataStore,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    599\u001b[0m     autoclose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    600\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m    601\u001b[0m     filename_or_obj \u001b[38;5;241m=\u001b[39m _normalize_path(filename_or_obj)\n\u001b[0;32m--> 602\u001b[0m     store \u001b[38;5;241m=\u001b[39m \u001b[43mNetCDF4DataStore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclobber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclobber\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiskless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiskless\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpersist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    614\u001b[0m     store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:400\u001b[0m, in \u001b[0;36mNetCDF4DataStore.open\u001b[0;34m(cls, filename, mode, format, group, clobber, diskless, persist, lock, lock_maker, autoclose)\u001b[0m\n\u001b[1;32m    394\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    395\u001b[0m     clobber\u001b[38;5;241m=\u001b[39mclobber, diskless\u001b[38;5;241m=\u001b[39mdiskless, persist\u001b[38;5;241m=\u001b[39mpersist, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m\n\u001b[1;32m    396\u001b[0m )\n\u001b[1;32m    397\u001b[0m manager \u001b[38;5;241m=\u001b[39m CachingFileManager(\n\u001b[1;32m    398\u001b[0m     netCDF4\u001b[38;5;241m.\u001b[39mDataset, filename, mode\u001b[38;5;241m=\u001b[39mmode, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[1;32m    399\u001b[0m )\n\u001b[0;32m--> 400\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:347\u001b[0m, in \u001b[0;36mNetCDF4DataStore.__init__\u001b[0;34m(self, manager, group, mode, lock, autoclose)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group \u001b[38;5;241m=\u001b[39m group\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m mode\n\u001b[0;32m--> 347\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mds\u001b[49m\u001b[38;5;241m.\u001b[39mdata_model\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds\u001b[38;5;241m.\u001b[39mfilepath()\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_remote \u001b[38;5;241m=\u001b[39m is_remote_uri(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:409\u001b[0m, in \u001b[0;36mNetCDF4DataStore.ds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mds\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 409\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:403\u001b[0m, in \u001b[0;36mNetCDF4DataStore._acquire\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_acquire\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager\u001b[38;5;241m.\u001b[39macquire_context(needs_lock) \u001b[38;5;28;01mas\u001b[39;00m root:\n\u001b[1;32m    404\u001b[0m         ds \u001b[38;5;241m=\u001b[39m _nc4_require_group(root, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode)\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xarray/backends/file_manager.py:199\u001b[0m, in \u001b[0;36mCachingFileManager.acquire_context\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21macquire_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;124;03m\"\"\"Context manager for acquiring a file.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m     file, cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire_with_cache_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneeds_lock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m file\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xarray/backends/file_manager.py:217\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    215\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    216\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode\n\u001b[0;32m--> 217\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_opener\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# ensure file doesn't get overridden when opened again\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2464\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2027\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno -72] NetCDF: Malformed or inaccessible DAP2 DDS or DAP4 DMR response: 'https://thredds.met.no/thredds/dodsC/fou-hi/barents_eps_eps/barents_eps_20230211T00Z.nc'"
     ]
    }
   ],
   "source": [
    "dataset0 = xr.open_dataset(\"https://thredds.met.no/thredds/dodsC/fou-hi/barents_eps_eps/barents_eps_20230211T00Z.nc\")\n",
    "dataset1 = xr.open_dataset(\"https://thredds.met.no/thredds/dodsC/fou-hi/barents_eps_eps/barents_eps_20230211T06Z.nc\")\n",
    "dataset2 = xr.open_dataset(\"https://thredds.met.no/thredds/dodsC/fou-hi/barents_eps_eps/barents_eps_20230211T12Z.nc\")\n",
    "dataset3 = xr.open_dataset(\"https://thredds.met.no/thredds/dodsC/fou-hi/barents_eps_eps/barents_eps_20230211T18Z.nc\")\n",
    "dataset4 = xr.open_dataset(\"https://thredds.met.no/thredds/dodsC/fou-hi/barents_eps_eps/barents_eps_20230213T00Z.nc\")\n",
    "dataset5 = xr.open_dataset(\"https://thredds.met.no/thredds/dodsC/fou-hi/barents_eps_eps/barents_eps_20230213T12Z.nc\")\n",
    "dataset6 = xr.open_dataset(\"https://thredds.met.no/thredds/dodsC/fou-hi/barents_eps_eps/barents_eps_20230213T18Z.nc\")\n",
    "dataset7 = xr.open_dataset(\"https://thredds.met.no/thredds/dodsC/fou-hi/barents_eps_eps/barents_eps_20230214T00Z.nc\")\n",
    "dataset8 = xr.open_dataset(\"https://thredds.met.no/thredds/dodsC/fou-hi/barents_eps_eps/barents_eps_20230214T06Z.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230c277e-1568-4d11-accf-5ca13b488c14",
   "metadata": {},
   "source": [
    "#### Variant with rolling evarage of 24 ensembles for the whole forecasting period with 6 hour ensembles update.\n",
    "Takes 4 preciding datasets for calculating mean 24 ensembles variables for the next 6 (or what is left before new update at 00,06,12,18) hours.\n",
    "At time of new forecast drops the most outdated 6 ensembles and add the most recent 6.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dce5e1-891d-46ff-891d-c06e9c903007",
   "metadata": {},
   "source": [
    "Description of the model:\n",
    "\n",
    "\n",
    "Sea ice drift forecasts were generated using a model that releases data in discrete files at four specific times daily: 00:00, 06:00, 12:00, and 18:00. Each of these files provides an hourly forecast of instantaneous sea ice drift measurements projected for the subsequent 66 hours. Crucially, every file contains data from six distinct ensemble predictions, summing to a total of 24 unique ensembles over the course of a day. The ensembles are systematically numbered from 0 to 23, with the 00:00 file encompassing ensembles 0-5, the 06:00 file including ensembles 6-11, the 12:00 file comprising ensembles 12-17, and the 18:00 file containing ensembles 18-23. At the model's finest resolution of 2.5 km, we observed that the sea ice drift is profoundly influenced by ocean surface features, which emerge as vortices in the drift data. It's worth noting that such vortices at this granularity are not commonly evident in real-world observations. Given the variability of these vortices across different ensembles, we employed an averaging technique across ensembles to decrease their influence and enhance the accuracy of our forecasts.\n",
    "\n",
    "\n",
    "\n",
    "* From the article (also it makes sense to take some general info from Victor's article)\n",
    "The sea ice model used in Barents-2.5 is the Los Alamos sea ice model (CICE), version 5.1 (Hunke et al., 2017). CICE describes both dynamic and thermodynamic processes, using a elastic-viscous-plastic (EVP) rheology (Hunke and Dukowicz, 1997, Röhrs et al, 2023). The ice velocity u is calculated from the sea ice momentum equation that account for air and water drag, Coriolis force, sea surface tilt, and the divergence of internal ice stress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14908692-7506-4f1d-bb52-8b78ff4bd781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'X' (DataArray)\n",
      "Stored 'Y' (DataArray)\n",
      "Stored 'model_proj4' (str)\n"
     ]
    }
   ],
   "source": [
    "# Extract grid variables\n",
    "\n",
    "X = dataset0['X']\n",
    "Y = dataset0['Y']\n",
    "lon = dataset0['lon']\n",
    "lat = dataset0['lat']\n",
    "model_proj4 = dataset0.projection_lambert.proj4\n",
    "\n",
    "%store X\n",
    "%store Y\n",
    "#%store lon\n",
    "#%store lat\n",
    "%store model_proj4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff6bc8e3-e77a-4298-85b5-705409905876",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [dataset0, dataset1, dataset2, dataset3, dataset4, dataset5, dataset6, dataset7, dataset8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85220ef6-04eb-4f67-99f0-877001a1f479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensembles\n",
      "dataset0: [0 1 2 3 4 5]\n",
      "dataset1: [ 6  7  8  9 10 11]\n",
      "dataset2: [12 13 14 15 16 17]\n",
      "dataset3: [18 19 20 21 22 23]\n",
      "dataset4: [0 1 2 3 4 5]\n",
      "dataset5: [12 13 14 15 16 17]\n",
      "dataset6: [18 19 20 21 22 23]\n",
      "dataset7: [0 1 2 3 4 5]\n",
      "dataset8: [ 6  7  8  9 10 11]\n",
      "Start time\n",
      "dataset0: 2023-02-11T00:00:00.000000000 - 2023-02-13T18:00:00.000000000, 66 forecast\n",
      "dataset1: 2023-02-11T06:00:00.000000000 - 2023-02-14T00:00:00.000000000, 66 forecast\n",
      "dataset2: 2023-02-11T12:00:00.000000000 - 2023-02-14T06:00:00.000000000, 66 forecast\n",
      "dataset3: 2023-02-11T18:00:00.000000000 - 2023-02-14T12:00:00.000000000, 66 forecast\n",
      "dataset4: 2023-02-13T00:00:00.000000000 - 2023-02-15T18:00:00.000000000, 66 forecast\n",
      "dataset5: 2023-02-13T12:00:00.000000000 - 2023-02-16T06:00:00.000000000, 66 forecast\n",
      "dataset6: 2023-02-13T18:00:00.000000000 - 2023-02-16T12:00:00.000000000, 66 forecast\n",
      "dataset7: 2023-02-14T00:00:00.000000000 - 2023-02-16T18:00:00.000000000, 66 forecast\n",
      "dataset8: 2023-02-14T06:00:00.000000000 - 2023-02-17T00:00:00.000000000, 66 forecast\n"
     ]
    }
   ],
   "source": [
    "print(\"Ensembles\")\n",
    "for idx, ensemble in enumerate(datasets):\n",
    "    print(f\"dataset{idx}: {datasets[idx].ensemble_member.values}\")\n",
    "print(\"Start time\")\n",
    "for idx, ensemble in enumerate(datasets):\n",
    "    print(f\"dataset{idx}: {datasets[idx].time[0].values} - {datasets[idx].time[-1].values}, {len(datasets[idx].time)- 1} forecast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dbbae19b-88df-4cbe-8191-29c1c0082318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAR1 time is 2023-02-12T08:01:51, Model start time for the time period is 2023-02-12T08:00:00\n",
      "SAR2 time is 2023-02-14T07:45:31, Model end time for the time period is 2023-02-14T08:00:00\n",
      "The duration of the time period is 48 whole hours\n"
     ]
    }
   ],
   "source": [
    "from model_data_prep import round_start_time, round_end_time, time_difference\n",
    "\n",
    "# SAR images timestamps\n",
    "t_sar1 = '2023-02-12T08:01:51'\n",
    "t_sar2 = '2023-02-14T07:45:31'\n",
    "\n",
    "# Rounding the SAR timestamps to align with the nearest whole hour of model timestamps\n",
    "t_start = round_start_time(t_sar1)\n",
    "t_end = round_end_time(t_sar2)\n",
    "\n",
    "print(f'SAR1 time is {t_sar1}, Model start time for the time period is {t_start}')\n",
    "print(f'SAR2 time is {t_sar2}, Model end time for the time period is {t_end}')\n",
    "\n",
    "\n",
    "# Extracting the model time variable data corresponding to the time period between the rounded start and end times\n",
    "time_period_barents = dataset3.time.sel(time=slice(t_start,t_end))\n",
    "\n",
    "print(f'The duration of the time period is {len(time_period_barents)-1} whole hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a36e2c8b-24c7-4258-aaec-f8718cb220fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "body[data-theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '►';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '▼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-index-preview {\n",
       "  grid-column: 2 / 5;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data,\n",
       ".xr-index-data-in:checked ~ .xr-index-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-index-name div,\n",
       ".xr-index-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2,\n",
       ".xr-no-icon {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.DataArray &#x27;time&#x27; (time: 49)&gt;\n",
       "array([&#x27;2023-02-12T08:00:00.000000000&#x27;, &#x27;2023-02-12T09:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-12T10:00:00.000000000&#x27;, &#x27;2023-02-12T11:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-12T12:00:00.000000000&#x27;, &#x27;2023-02-12T13:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-12T14:00:00.000000000&#x27;, &#x27;2023-02-12T15:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-12T16:00:00.000000000&#x27;, &#x27;2023-02-12T17:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-12T18:00:00.000000000&#x27;, &#x27;2023-02-12T19:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-12T20:00:00.000000000&#x27;, &#x27;2023-02-12T21:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-12T22:00:00.000000000&#x27;, &#x27;2023-02-12T23:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-13T00:00:00.000000000&#x27;, &#x27;2023-02-13T01:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-13T02:00:00.000000000&#x27;, &#x27;2023-02-13T03:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-13T04:00:00.000000000&#x27;, &#x27;2023-02-13T05:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-13T06:00:00.000000000&#x27;, &#x27;2023-02-13T07:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-13T08:00:00.000000000&#x27;, &#x27;2023-02-13T09:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-13T10:00:00.000000000&#x27;, &#x27;2023-02-13T11:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-13T12:00:00.000000000&#x27;, &#x27;2023-02-13T13:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-13T14:00:00.000000000&#x27;, &#x27;2023-02-13T15:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-13T16:00:00.000000000&#x27;, &#x27;2023-02-13T17:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-13T18:00:00.000000000&#x27;, &#x27;2023-02-13T19:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-13T20:00:00.000000000&#x27;, &#x27;2023-02-13T21:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-13T22:00:00.000000000&#x27;, &#x27;2023-02-13T23:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-14T00:00:00.000000000&#x27;, &#x27;2023-02-14T01:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-14T02:00:00.000000000&#x27;, &#x27;2023-02-14T03:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-14T04:00:00.000000000&#x27;, &#x27;2023-02-14T05:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-14T06:00:00.000000000&#x27;, &#x27;2023-02-14T07:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-14T08:00:00.000000000&#x27;], dtype=&#x27;datetime64[ns]&#x27;)\n",
       "Coordinates:\n",
       "  * time     (time) datetime64[ns] 2023-02-12T08:00:00 ... 2023-02-14T08:00:00\n",
       "Attributes:\n",
       "    long_name:      time since initialization\n",
       "    field:          time, scalar, series\n",
       "    axis:           T\n",
       "    standard_name:  time\n",
       "    _ChunkSizes:    1</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.DataArray</div><div class='xr-array-name'>'time'</div><ul class='xr-dim-list'><li><span class='xr-has-index'>time</span>: 49</li></ul></div><ul class='xr-sections'><li class='xr-section-item'><div class='xr-array-wrap'><input id='section-ab571598-b257-4272-83af-6ac5468861d2' class='xr-array-in' type='checkbox' checked><label for='section-ab571598-b257-4272-83af-6ac5468861d2' title='Show/hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-array-preview xr-preview'><span>2023-02-12T08:00:00 2023-02-12T09:00:00 ... 2023-02-14T08:00:00</span></div><div class='xr-array-data'><pre>array([&#x27;2023-02-12T08:00:00.000000000&#x27;, &#x27;2023-02-12T09:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-12T10:00:00.000000000&#x27;, &#x27;2023-02-12T11:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-12T12:00:00.000000000&#x27;, &#x27;2023-02-12T13:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-12T14:00:00.000000000&#x27;, &#x27;2023-02-12T15:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-12T16:00:00.000000000&#x27;, &#x27;2023-02-12T17:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-12T18:00:00.000000000&#x27;, &#x27;2023-02-12T19:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-12T20:00:00.000000000&#x27;, &#x27;2023-02-12T21:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-12T22:00:00.000000000&#x27;, &#x27;2023-02-12T23:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-13T00:00:00.000000000&#x27;, &#x27;2023-02-13T01:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-13T02:00:00.000000000&#x27;, &#x27;2023-02-13T03:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-13T04:00:00.000000000&#x27;, &#x27;2023-02-13T05:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-13T06:00:00.000000000&#x27;, &#x27;2023-02-13T07:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-13T08:00:00.000000000&#x27;, &#x27;2023-02-13T09:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-13T10:00:00.000000000&#x27;, &#x27;2023-02-13T11:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-13T12:00:00.000000000&#x27;, &#x27;2023-02-13T13:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-13T14:00:00.000000000&#x27;, &#x27;2023-02-13T15:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-13T16:00:00.000000000&#x27;, &#x27;2023-02-13T17:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-13T18:00:00.000000000&#x27;, &#x27;2023-02-13T19:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-13T20:00:00.000000000&#x27;, &#x27;2023-02-13T21:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-13T22:00:00.000000000&#x27;, &#x27;2023-02-13T23:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-14T00:00:00.000000000&#x27;, &#x27;2023-02-14T01:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-14T02:00:00.000000000&#x27;, &#x27;2023-02-14T03:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-14T04:00:00.000000000&#x27;, &#x27;2023-02-14T05:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-14T06:00:00.000000000&#x27;, &#x27;2023-02-14T07:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-14T08:00:00.000000000&#x27;], dtype=&#x27;datetime64[ns]&#x27;)</pre></div></div></li><li class='xr-section-item'><input id='section-0d435a48-9cee-45af-af42-81d11ccb8a49' class='xr-section-summary-in' type='checkbox'  checked><label for='section-0d435a48-9cee-45af-af42-81d11ccb8a49' class='xr-section-summary' >Coordinates: <span>(1)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>time</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>2023-02-12T08:00:00 ... 2023-02-...</div><input id='attrs-b539a67c-ccdc-4b65-b426-fdf5e0922760' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-b539a67c-ccdc-4b65-b426-fdf5e0922760' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-72b6627a-a707-4426-8bb1-d573bf1d75bc' class='xr-var-data-in' type='checkbox'><label for='data-72b6627a-a707-4426-8bb1-d573bf1d75bc' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>time since initialization</dd><dt><span>field :</span></dt><dd>time, scalar, series</dd><dt><span>axis :</span></dt><dd>T</dd><dt><span>standard_name :</span></dt><dd>time</dd><dt><span>_ChunkSizes :</span></dt><dd>1</dd></dl></div><div class='xr-var-data'><pre>array([&#x27;2023-02-12T08:00:00.000000000&#x27;, &#x27;2023-02-12T09:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-12T10:00:00.000000000&#x27;, &#x27;2023-02-12T11:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-12T12:00:00.000000000&#x27;, &#x27;2023-02-12T13:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-12T14:00:00.000000000&#x27;, &#x27;2023-02-12T15:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-12T16:00:00.000000000&#x27;, &#x27;2023-02-12T17:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-12T18:00:00.000000000&#x27;, &#x27;2023-02-12T19:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-12T20:00:00.000000000&#x27;, &#x27;2023-02-12T21:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-12T22:00:00.000000000&#x27;, &#x27;2023-02-12T23:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-13T00:00:00.000000000&#x27;, &#x27;2023-02-13T01:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-13T02:00:00.000000000&#x27;, &#x27;2023-02-13T03:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-13T04:00:00.000000000&#x27;, &#x27;2023-02-13T05:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-13T06:00:00.000000000&#x27;, &#x27;2023-02-13T07:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-13T08:00:00.000000000&#x27;, &#x27;2023-02-13T09:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-13T10:00:00.000000000&#x27;, &#x27;2023-02-13T11:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-13T12:00:00.000000000&#x27;, &#x27;2023-02-13T13:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-13T14:00:00.000000000&#x27;, &#x27;2023-02-13T15:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-13T16:00:00.000000000&#x27;, &#x27;2023-02-13T17:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-13T18:00:00.000000000&#x27;, &#x27;2023-02-13T19:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-13T20:00:00.000000000&#x27;, &#x27;2023-02-13T21:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-13T22:00:00.000000000&#x27;, &#x27;2023-02-13T23:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-14T00:00:00.000000000&#x27;, &#x27;2023-02-14T01:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-14T02:00:00.000000000&#x27;, &#x27;2023-02-14T03:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-14T04:00:00.000000000&#x27;, &#x27;2023-02-14T05:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-14T06:00:00.000000000&#x27;, &#x27;2023-02-14T07:00:00.000000000&#x27;,\n",
       "       &#x27;2023-02-14T08:00:00.000000000&#x27;], dtype=&#x27;datetime64[ns]&#x27;)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-76afb34b-54e3-471c-b20a-fb8e09be8587' class='xr-section-summary-in' type='checkbox'  ><label for='section-76afb34b-54e3-471c-b20a-fb8e09be8587' class='xr-section-summary' >Indexes: <span>(1)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-index-name'><div>time</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-3e56d12d-c136-4b4b-a8e6-803eb03bf2e9' class='xr-index-data-in' type='checkbox'/><label for='index-3e56d12d-c136-4b4b-a8e6-803eb03bf2e9' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(DatetimeIndex([&#x27;2023-02-12 08:00:00&#x27;, &#x27;2023-02-12 09:00:00&#x27;,\n",
       "               &#x27;2023-02-12 10:00:00&#x27;, &#x27;2023-02-12 11:00:00&#x27;,\n",
       "               &#x27;2023-02-12 12:00:00&#x27;, &#x27;2023-02-12 13:00:00&#x27;,\n",
       "               &#x27;2023-02-12 14:00:00&#x27;, &#x27;2023-02-12 15:00:00&#x27;,\n",
       "               &#x27;2023-02-12 16:00:00&#x27;, &#x27;2023-02-12 17:00:00&#x27;,\n",
       "               &#x27;2023-02-12 18:00:00&#x27;, &#x27;2023-02-12 19:00:00&#x27;,\n",
       "               &#x27;2023-02-12 20:00:00&#x27;, &#x27;2023-02-12 21:00:00&#x27;,\n",
       "               &#x27;2023-02-12 22:00:00&#x27;, &#x27;2023-02-12 23:00:00&#x27;,\n",
       "               &#x27;2023-02-13 00:00:00&#x27;, &#x27;2023-02-13 01:00:00&#x27;,\n",
       "               &#x27;2023-02-13 02:00:00&#x27;, &#x27;2023-02-13 03:00:00&#x27;,\n",
       "               &#x27;2023-02-13 04:00:00&#x27;, &#x27;2023-02-13 05:00:00&#x27;,\n",
       "               &#x27;2023-02-13 06:00:00&#x27;, &#x27;2023-02-13 07:00:00&#x27;,\n",
       "               &#x27;2023-02-13 08:00:00&#x27;, &#x27;2023-02-13 09:00:00&#x27;,\n",
       "               &#x27;2023-02-13 10:00:00&#x27;, &#x27;2023-02-13 11:00:00&#x27;,\n",
       "               &#x27;2023-02-13 12:00:00&#x27;, &#x27;2023-02-13 13:00:00&#x27;,\n",
       "               &#x27;2023-02-13 14:00:00&#x27;, &#x27;2023-02-13 15:00:00&#x27;,\n",
       "               &#x27;2023-02-13 16:00:00&#x27;, &#x27;2023-02-13 17:00:00&#x27;,\n",
       "               &#x27;2023-02-13 18:00:00&#x27;, &#x27;2023-02-13 19:00:00&#x27;,\n",
       "               &#x27;2023-02-13 20:00:00&#x27;, &#x27;2023-02-13 21:00:00&#x27;,\n",
       "               &#x27;2023-02-13 22:00:00&#x27;, &#x27;2023-02-13 23:00:00&#x27;,\n",
       "               &#x27;2023-02-14 00:00:00&#x27;, &#x27;2023-02-14 01:00:00&#x27;,\n",
       "               &#x27;2023-02-14 02:00:00&#x27;, &#x27;2023-02-14 03:00:00&#x27;,\n",
       "               &#x27;2023-02-14 04:00:00&#x27;, &#x27;2023-02-14 05:00:00&#x27;,\n",
       "               &#x27;2023-02-14 06:00:00&#x27;, &#x27;2023-02-14 07:00:00&#x27;,\n",
       "               &#x27;2023-02-14 08:00:00&#x27;],\n",
       "              dtype=&#x27;datetime64[ns]&#x27;, name=&#x27;time&#x27;, freq=None))</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-1c9ce6ae-2376-4c46-9463-133324182907' class='xr-section-summary-in' type='checkbox'  checked><label for='section-1c9ce6ae-2376-4c46-9463-133324182907' class='xr-section-summary' >Attributes: <span>(5)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>time since initialization</dd><dt><span>field :</span></dt><dd>time, scalar, series</dd><dt><span>axis :</span></dt><dd>T</dd><dt><span>standard_name :</span></dt><dd>time</dd><dt><span>_ChunkSizes :</span></dt><dd>1</dd></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.DataArray 'time' (time: 49)>\n",
       "array(['2023-02-12T08:00:00.000000000', '2023-02-12T09:00:00.000000000',\n",
       "       '2023-02-12T10:00:00.000000000', '2023-02-12T11:00:00.000000000',\n",
       "       '2023-02-12T12:00:00.000000000', '2023-02-12T13:00:00.000000000',\n",
       "       '2023-02-12T14:00:00.000000000', '2023-02-12T15:00:00.000000000',\n",
       "       '2023-02-12T16:00:00.000000000', '2023-02-12T17:00:00.000000000',\n",
       "       '2023-02-12T18:00:00.000000000', '2023-02-12T19:00:00.000000000',\n",
       "       '2023-02-12T20:00:00.000000000', '2023-02-12T21:00:00.000000000',\n",
       "       '2023-02-12T22:00:00.000000000', '2023-02-12T23:00:00.000000000',\n",
       "       '2023-02-13T00:00:00.000000000', '2023-02-13T01:00:00.000000000',\n",
       "       '2023-02-13T02:00:00.000000000', '2023-02-13T03:00:00.000000000',\n",
       "       '2023-02-13T04:00:00.000000000', '2023-02-13T05:00:00.000000000',\n",
       "       '2023-02-13T06:00:00.000000000', '2023-02-13T07:00:00.000000000',\n",
       "       '2023-02-13T08:00:00.000000000', '2023-02-13T09:00:00.000000000',\n",
       "       '2023-02-13T10:00:00.000000000', '2023-02-13T11:00:00.000000000',\n",
       "       '2023-02-13T12:00:00.000000000', '2023-02-13T13:00:00.000000000',\n",
       "       '2023-02-13T14:00:00.000000000', '2023-02-13T15:00:00.000000000',\n",
       "       '2023-02-13T16:00:00.000000000', '2023-02-13T17:00:00.000000000',\n",
       "       '2023-02-13T18:00:00.000000000', '2023-02-13T19:00:00.000000000',\n",
       "       '2023-02-13T20:00:00.000000000', '2023-02-13T21:00:00.000000000',\n",
       "       '2023-02-13T22:00:00.000000000', '2023-02-13T23:00:00.000000000',\n",
       "       '2023-02-14T00:00:00.000000000', '2023-02-14T01:00:00.000000000',\n",
       "       '2023-02-14T02:00:00.000000000', '2023-02-14T03:00:00.000000000',\n",
       "       '2023-02-14T04:00:00.000000000', '2023-02-14T05:00:00.000000000',\n",
       "       '2023-02-14T06:00:00.000000000', '2023-02-14T07:00:00.000000000',\n",
       "       '2023-02-14T08:00:00.000000000'], dtype='datetime64[ns]')\n",
       "Coordinates:\n",
       "  * time     (time) datetime64[ns] 2023-02-12T08:00:00 ... 2023-02-14T08:00:00\n",
       "Attributes:\n",
       "    long_name:      time since initialization\n",
       "    field:          time, scalar, series\n",
       "    axis:           T\n",
       "    standard_name:  time\n",
       "    _ChunkSizes:    1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_period_barents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a205648-5d9e-40f3-87b0-ff0b6b913a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time difference between SAR1 and the start of the model time period is 111 seconds (1.85 minutes).\n",
      "Time difference between SAR2 and the end of the model time period is 869 seconds (14.483 minutes).\n",
      "Total time difference between SAR1 and SAR2 images is 171820 seconds (1.989 days).\n",
      "Stored 'time_diff_start' (int64)\n",
      "Stored 'time_diff_end' (int64)\n",
      "Stored 'total_time_diff' (int64)\n",
      "Stored 'time_period_barents' (DataArray)\n"
     ]
    }
   ],
   "source": [
    "# Store variables\n",
    "time_diff_start, time_diff_end, total_time_diff = time_difference(t_sar1, t_sar2, time_period_barents)\n",
    "\n",
    "%store time_diff_start\n",
    "%store time_diff_end\n",
    "%store total_time_diff\n",
    "%store time_period_barents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86256f81-f121-4502-a884-717f28584f33",
   "metadata": {},
   "source": [
    "Methodology: Rolling Averaging of Ensemble Ice Drift Velocities\n",
    "\n",
    "The goal of this methodology is to compute the average ice drift velocities, specifically ice_u and ice_v, over a designated time range using a sliding window of ensemble forecasts. This approach seeks to always use the latest available ensemble forecasts to provide an updated and more accurate representation of ice drift velocities.\n",
    "\n",
    "Defining the Forecast Time Range:\n",
    "The start and end times of the forecast period are specified. The period is then divided into hourly intervals using the pd.date_range() function, producing a sequence of timestamps.\n",
    "\n",
    "Selection of Relevant Datasets:\n",
    "For each hourly timestamp:\n",
    "\n",
    "All datasets that have forecast data encompassing the current timestamp are identified.\n",
    "These datasets are sorted based on their starting forecast time.\n",
    "The four most recent datasets are selected to ensure the usage of the latest ensemble forecasts.\n",
    "Data Extraction and Concatenation:\n",
    "For each of the selected datasets, the ice_u and ice_v data corresponding to the current timestamp are extracted. Additionally, a spatial subset of the data is chosen based on specified row (r) and column (c) indices. To handle missing data, any NaN values are retained by using the fillna(np.nan) function. The extracted ensemble data from all four datasets are concatenated along a new dimension named ensemble_member, resulting in a combined dataset with data from 24 ensemble members.\n",
    "\n",
    "Averaging Over Ensembles:\n",
    "The concatenated ensemble data are then averaged over the ensemble_member dimension, yielding a single averaged value for ice_u and ice_v for each hourly timestamp.\n",
    "\n",
    "Creation of Time Series DataArrays:\n",
    "The hourly averaged ice_u and ice_v values are concatenated along the time dimension, producing two continuous time series DataArray objects that represent the average ice drift velocities over the entire forecast period.\n",
    "\n",
    "The rolling_avg_24_ensembles function implements the aforementioned methodology, taking as input a list of ensemble datasets and the desired start and end times of the forecast period. It returns two DataArray objects containing the averaged ice drift velocities (avg_ice_u_da and avg_ice_v_da) for the entire period. This approach ensures a continuous and updated representation of the average ice drift velocities by always considering the latest ensemble forecasts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f106ab99-2f0f-4ce9-a225-56677911f77f",
   "metadata": {},
   "source": [
    "Short version:\n",
    "To provide a continuous and updated representation of ice drift velocities, we employed a rolling averaging approach. For each hour within a designated forecast period, we sourced data from the four most recent ensemble forecast datasets available at that time. By prioritizing the latest ensemble forecasts, we ensured the inclusion of the most up-to-date predictions in our analysis. For each hourly timestep, the ice drift velocities were extracted from these datasets, spatially subsetted, and then averaged across all 24 ensemble members. This process generated a time series of average ice drift velocities spanning the entire forecast period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ba6bec-bf26-4fa8-97ff-bc899b1bcc2e",
   "metadata": {},
   "source": [
    "##### Calculating rolling average for full and subset gtids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3524740-3464-4d9a-bda6-44d538576b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_data_prep import rolling_avg_24_ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aca485d7-13af-419f-9fd4-2e8e067abb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the whole extent\n",
    "avg_ice_u, avg_ice_v = rolling_avg_24_ensembles(datasets, t_start, t_end, subset = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11be9d2e-71a3-4ee1-89cb-0aa535b58187",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Calculate mean hourly (NOT cumulative) drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d14c67-9b0d-4a76-9af2-4df0fdaaacf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_data_prep import non_cumulative_ice_displacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b8fb16-e2e2-4431-9dac-804438332b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy, hourly_mean_disp_u, hourly_mean_disp_v =  non_cumulative_ice_displacement(X, Y, avg_ice_u, avg_ice_v, time_period_barents, time_diff_start, time_diff_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15566de5-081b-4919-93dc-631ca39c4d0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Saving speed as pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93610b31-2e86-49f0-83be-fee762b9d2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_path_u = \"/home/jovyan/experiment_data/2022-2023_48h_experiment/drift_output/14_16_feb/barents_24ens_14_16feb_hourly_ice_u.pkl\"\n",
    "pickle_path_v = \"/home/jovyan/experiment_data/2022-2023_48h_experiment/drift_output/14_16_feb/barents_24ens_14_16feb_hourly_ice_v.pkl\"\n",
    "# Save the list\n",
    "with open(pickle_path_u, \"wb\") as f:\n",
    "    pickle.dump(hourly_mean_disp_u, f)\n",
    "    \n",
    "with open(pickle_path_v, \"wb\") as f:\n",
    "    pickle.dump(hourly_mean_disp_v, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0922a05f-cc61-47ce-b3f1-4ded520b188d",
   "metadata": {},
   "source": [
    "##### Calculating cummulative (integrated) drift for the full extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf9ef947-1cba-4ece-bfba-78578ca4fd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_data_prep import cumulative_ice_displacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b631fa2-847d-4ef7-a5d0-9a1d700c719b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 hour done\n",
      "2 hour done\n",
      "3 hour done\n",
      "4 hour done\n",
      "5 hour done\n",
      "6 hour done\n",
      "7 hour done\n",
      "8 hour done\n",
      "9 hour done\n",
      "10 hour done\n",
      "11 hour done\n",
      "12 hour done\n",
      "13 hour done\n",
      "14 hour done\n",
      "15 hour done\n",
      "16 hour done\n",
      "17 hour done\n",
      "18 hour done\n",
      "19 hour done\n",
      "20 hour done\n",
      "21 hour done\n",
      "22 hour done\n",
      "23 hour done\n",
      "24 hour done\n",
      "25 hour done\n",
      "26 hour done\n",
      "27 hour done\n",
      "28 hour done\n",
      "29 hour done\n",
      "30 hour done\n",
      "31 hour done\n",
      "32 hour done\n",
      "33 hour done\n",
      "34 hour done\n",
      "35 hour done\n",
      "36 hour done\n",
      "37 hour done\n",
      "38 hour done\n",
      "39 hour done\n",
      "40 hour done\n",
      "41 hour done\n",
      "42 hour done\n",
      "43 hour done\n",
      "44 hour done\n",
      "45 hour done\n",
      "46 hour done\n",
      "47 hour done\n",
      "48 hour done\n"
     ]
    }
   ],
   "source": [
    "# Calculate cumulative drift for the whole grid \n",
    "xx_b, yy_b, cum_dx_b, cum_dy_b = cumulative_ice_displacement(X, Y, avg_ice_u, avg_ice_v, time_period_barents, time_diff_start, time_diff_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cfbd08-931e-4c64-9a18-dca9ed0d1f5f",
   "metadata": {},
   "source": [
    "##### Saving cum displacements as pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d0c274-43fe-457c-b8a7-9f0686c134af",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_path_u = \"/home/jovyan/experiment_data/2022-2023_48h_experiment/drift_output/14_16_feb/barents_24ens_14_16feb_cum_ice_u.pkl\"\n",
    "pickle_path_v = \"/home/jovyan/experiment_data/2022-2023_48h_experiment/drift_output/14_16_feb/barents_24ens_14_16feb_cum_ice_v.pkl\"\n",
    "# Save the list\n",
    "with open(pickle_path_u, \"wb\") as f:\n",
    "    pickle.dump(cum_dx_b, f)\n",
    "    \n",
    "with open(pickle_path_v, \"wb\") as f:\n",
    "    pickle.dump(cum_dy_b, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0412abb-5ff4-42bc-a240-d5bb8d1d3958",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Calculating cummulative (integrated) drift for the subset extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7664c619-fa6a-43cb-93b1-acb82399553a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import variables for extracting subsets from SAR1-SAR2_drift.ipynb\n",
    "%store -r min_row\n",
    "%store -r max_row\n",
    "%store -r min_col\n",
    "%store -r max_col\n",
    "%store -r X_subset\n",
    "%store -r Y_subset\n",
    "%store -r lon_subset\n",
    "%store -r lat_subset\n",
    "\n",
    "%store -r lon\n",
    "%store -r lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81728c28-145c-488e-9a8d-38789716a72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "barents_model_drift_path = \"/home/jovyan/experiment_data/2022-2023_48h_experiment/drift_output/12_14_feb/barents_24ens_cum_disp_14-16feb.csv\"\n",
    "with open(barents_model_drift_path) as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    # check if the file has a header row\n",
    "    has_header = csv.Sniffer().has_header(csvfile.read(1024))\n",
    "    csvfile.seek(0)  # reset file pointer\n",
    "    #skip header row if it exists\n",
    "    if has_header:\n",
    "        next(csvreader)\n",
    "    displacements = np.array([row for row in csvreader]).astype(float)\n",
    "    cum_dx_b = displacements[:, [2]] \n",
    "    cum_dy_b = displacements[:, [3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dfe141a-f165-44de-9e61-4ae745ad3ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model_shape = (949, 739) #lon.shape\n",
    "cum_dx_b_total = np.reshape(cum_dx_b, full_model_shape)\n",
    "cum_dy_b_total = np.reshape(cum_dy_b, full_model_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97dc8b6d-38a9-4e54-b0cf-6504100306bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_dx_b_subset = cum_dx_b_total[min_row:max_row+1, min_col:max_col+1]\n",
    "cum_dy_b_subset = cum_dy_b_total[min_row:max_row+1, min_col:max_col+1]\n",
    "lon_subset = lon[min_row:max_row+1, min_col:max_col+1]\n",
    "lat_subset = lat[min_row:max_row+1, min_col:max_col+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8383412-2598-41b0-b185-b69a8b39653d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((949, 739), (283, 219))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cum_dy_b_total.shape, cum_dy_b_subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fddba10f-e598-49f5-bc81-f78a30cc1509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(283, 219) (283, 219)\n",
      "(61977,) (61977,)\n"
     ]
    }
   ],
   "source": [
    "from osgeo import osr\n",
    "# Define the target spatial reference (WGS 84)\n",
    "target_srs = osr.SpatialReference()\n",
    "target_srs.ImportFromEPSG(4326)\n",
    "\n",
    "# Define the source spatial reference (model's projection)\n",
    "model_srs = osr.SpatialReference()\n",
    "model_srs.ImportFromProj4(model_proj4)\n",
    "\n",
    "# Create the coordinate transformation\n",
    "transformation = osr.CoordinateTransformation(model_srs, target_srs)\n",
    "\n",
    "# Calculate teh final coordinates x2, y2\n",
    "\n",
    "x1, y1 = np.meshgrid(X_subset, Y_subset)\n",
    "print(x1.shape, y1.shape)\n",
    "x1, y1 = x1.flatten(), y1.flatten()\n",
    "print(x1.shape, y1.shape)\n",
    "dx = cum_dx_b_subset.flatten()\n",
    "dy = cum_dy_b_subset.flatten()\n",
    "\n",
    "x2 = x1 + dx\n",
    "y2 = y1 + dy\n",
    "\n",
    "# Stack the coordinates for transformation\n",
    "points = np.stack((x2, y2), axis=1)\n",
    "# Apply the coordinate transformation\n",
    "transformed_points = transformation.TransformPoints(points)\n",
    "\n",
    "\n",
    "# Extract longitude and latitude\n",
    "lat2_subset = np.array([point[0] for point in transformed_points])\n",
    "lon2_subset = np.array([point[1] for point in transformed_points])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2bf780-14eb-4a40-a0cf-fe124ea613a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to change the function (add r and c to make it work via imported function)\n",
    "# For the subset if provided\n",
    "avg_ice_u_subset, avg_ice_v_subset = rolling_avg_24_ensembles(datasets, t_start, t_end, subset = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425add94-84c2-4e5d-88af-d93cfdaafac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cumulative drift for subset grid \n",
    "xx_b_subset, yy_b_subset, cum_dx_b_subset, cum_dy_b_subset = cumulative_ice_displacement(X_subset,Y_subset, avg_ice_u_subset, avg_ice_v_subset, time_period_barents, time_diff_start,time_diff_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76367d8e-b8f3-45a6-8b75-df71710df180",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store variables\n",
    "%store xx_b\n",
    "%store yy_b\n",
    "%store cum_dx_b\n",
    "%store cum_dy_b \n",
    "\n",
    "'''\n",
    "%store xx_b_subset\n",
    "%store yy_b_subset\n",
    "%store cum_dx_b_subset\n",
    "%store cum_dy_b_subset \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c78aa71-17a1-4853-884e-b83272deed72",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Calculating average concentration for t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "578ccb89-00e8-4577-ac56-ff6a1ab84557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset0.ensemble_member.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dcc3b0-6154-4fa3-8fb8-b0e2c94ba88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_concentration(datasets, time_period):\n",
    "\n",
    "    available_datasets = [ds for ds in datasets if (ds.time[0].values <= time_period[0])]\n",
    "    print(f'{len(available_datasets)} avalible datasets')\n",
    "    conc_t_start_ens = xr.concat([ds.sel(time=time_period[0]).ice_concentration.fillna(np.nan) for ds in available_datasets], dim='ensemble_member')\n",
    "    print(f'ensembles for averaging are {conc_t_start_ens.ensemble_member.values}')\n",
    "    avg_conc_t_start = conc_t_start_ens.mean(dim='ensemble_member')\n",
    "    \n",
    "    return avg_conc_t_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bde15b-2023-4b7f-8ef5-f43ee82e5848",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_conc_t_start = avg_concentration(datasets, time_period_barents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e28ebf4-3611-46ed-be40-be5ae4ac4513",
   "metadata": {},
   "outputs": [],
   "source": [
    "feb1214_conc = avg_conc_t_start\n",
    "%store feb1214_conc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08396d46-42dd-40d7-a083-e58812fc1dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_conc_t_start.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67279cc7-0a0d-4068-a771-d1ce531db6e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Export cumulative drift to csv  (For chalmers)\n",
    "(alternative way is just to pass it as stored variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78be4402-9dab-4f68-ba16-710f62b993c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset grid\n",
    "output_dir_subset_comp = r'/home/jovyan/experiment_data/2022-2023_48h_experiment/drift_output/12_14_feb/'\n",
    "\n",
    "lon1 = lon_subset.data.flatten()\n",
    "lat1 = lat_subset.data.flatten()\n",
    "\n",
    "points=zip(lon1, lat1, lon2_subset, lat2_subset)\n",
    "header=['lon1','lat1', 'lon2','lat2']\n",
    "\n",
    "#file_name = f'24ens_roll_cum_displacement_{filename_t_start}-{filename_t_end}.csv'\n",
    "file_name = f'barents_model_24ens_cum_disp_for_chalmers.csv'\n",
    "file_path = os.path.join(output_dir_subset_comp, file_name)\n",
    "\n",
    "with open(file_path, 'w', newline='') as csv_file:\n",
    "            out = csv.writer(csv_file, delimiter=',')\n",
    "            out.writerow(header)\n",
    "            out.writerows(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92671786-4060-4400-b13c-2af047296a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full grid\n",
    "output_dir_subset_comp =  r'/home/jovyan/experiment_data/2022-2023_48h_experiment/drift_output/14_16_feb'\n",
    "\n",
    "lon1 = lon.data.flatten()\n",
    "lat1 = lat.data.flatten()\n",
    "\n",
    "points=zip(lon1, lat1, cum_dx_b[-1], cum_dy_b[-1])\n",
    "header=['lon1','lat1', 'du','dv']\n",
    "\n",
    "file_name = f'barents_24ens_cum_disp_14-16feb.csv'\n",
    "file_path = os.path.join(output_dir_subset_comp, file_name)\n",
    "\n",
    "with open(file_path, 'w', newline='') as csv_file:\n",
    "            out = csv.writer(csv_file, delimiter=',')\n",
    "            out.writerow(header)\n",
    "            out.writerows(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6853490-f436-45fd-9eb6-b317acb5ac90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset grid\n",
    "output_dir_subset_comp =  r'/home/jovyan/data/model_data_process_output/for_comparison/subset_comparison_with_sar'\n",
    "\n",
    "lon1 = lon_subset.data.flatten()\n",
    "lat1 = lat_subset.data.flatten()\n",
    "\n",
    "points=zip(lon1, lat1, cum_dx_b_subset[-1], cum_dy_b_subset[-1])\n",
    "header=['lon1','lat1', 'du','dv']\n",
    "\n",
    "#file_name = f'24ens_roll_cum_displacement_{filename_t_start}-{filename_t_end}.csv'\n",
    "file_name = f'barents_subset_24ens_cum_displacement.csv'\n",
    "file_path = os.path.join(output_dir_subset_comp, file_name)\n",
    "\n",
    "with open(file_path, 'w', newline='') as csv_file:\n",
    "            out = csv.writer(csv_file, delimiter=',')\n",
    "            out.writerow(header)\n",
    "            out.writerows(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a114d9a0-75dc-475f-82c9-21d2dcb4b191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export for Chalmers forecasting (with buffer)\n",
    "\n",
    "'''\n",
    "need to add but it is not needed for cumulative drift\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4854db88-59b8-40f5-af3b-a8f7128a0575",
   "metadata": {},
   "source": [
    "##### Calculation of variations for estimation of realibility of instantanious values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b672af8-48eb-4cef-9e1b-b597e32dad00",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_velocity_roll = np.sqrt(avg_ice_u_subset**2 + avg_ice_v_subset**2)\n",
    "total_velocity_roll.name = \"total_velocity\"\n",
    "total_velocity_roll[0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c4e884-01c9-4977-8891-cc63817b6e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity_variation_roll = total_velocity_roll.std(dim='time')\n",
    "velocity_variation_roll.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe10f991-98e1-4447-affe-254a5ca81e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_velocity_roll = total_velocity_roll.mean(dim='time')\n",
    "CV_roll = (velocity_variation_roll / mean_velocity_roll) * 100\n",
    "CV_roll.plot(vmin=0, vmax=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97286c9-30ed-4981-9118-ca0168986fbf",
   "metadata": {},
   "source": [
    "Interesting the way rolling averaging provide less variation for gradient ice drift areas and more variation within vortecies..that makes sence that vortecies change a lot from ensemble to ensemble and every 6 hour. It probably means that they not that connected for one ensemble after 24 hours.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f912dd4c-1bee-4ab1-ae86-79fdda1bacfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
