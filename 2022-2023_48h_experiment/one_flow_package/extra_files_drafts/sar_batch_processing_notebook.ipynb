{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9429456e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./modules\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6834b5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import general modules\n",
    "from nansat import Nansat, Domain, NSR\n",
    "import os \n",
    "\n",
    "# Import temporal modules needed for testing plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# Import SAR forecasting modules\n",
    "import config\n",
    "import s1_preparation\n",
    "import domains_preparation\n",
    "import SAR1_SAR2_drift_retrivial\n",
    "import warping_with_domain\n",
    "\n",
    "# Import variables\n",
    "from config import path_to_HH_files, path_to_HV_files, safe_folder \n",
    "from config import output_folder, input_folder\n",
    "from config import S1_prod_regex, S1_safe_regex\n",
    "from config import lon, lat, X, Y, proj4, srs\n",
    "\n",
    "# For cleaning up memory\n",
    "import gc\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "import cProfile\n",
    "import pstats\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28eeefbd-e6c3-4bce-bc20-4d1a0fb8b528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair 1:\n",
      "SAR1: S1A_EW_GRDM_1SDH_20221120T080155_20221120T080259_045975_05805E_51E1.SAFE \n",
      "timestamp: 2022-11-20 08:01:55\n",
      "SAR2: S1A_EW_GRDM_1SDH_20221122T074535_20221122T074639_046004_05816B_9FC9.SAFE \n",
      "timestamp: 2022-11-22 07:45:35\n",
      "Pair 2:\n",
      "SAR1: S1A_EW_GRDM_1SDH_20221207T081027_20221207T081131_046223_0588F0_3669.SAFE \n",
      "timestamp: 2022-12-07 08:10:27\n",
      "SAR2: S1A_EW_GRDM_1SDH_20221209T075358_20221209T075502_046252_0589D9_984C.SAFE \n",
      "timestamp: 2022-12-09 07:53:58\n",
      "Pair 3:\n",
      "SAR1: S1A_EW_GRDM_1SDH_20221226T080153_20221226T080257_046500_059247_AF72.SAFE \n",
      "timestamp: 2022-12-26 08:01:53\n",
      "SAR2: S1A_EW_GRDM_1SDH_20221228T074533_20221228T074637_046529_05934F_9B36.SAFE \n",
      "timestamp: 2022-12-28 07:45:33\n",
      "Pair 4:\n",
      "SAR1: S1A_EW_GRDM_1SDH_20230107T080152_20230107T080257_046675_059829_112D.SAFE \n",
      "timestamp: 2023-01-07 08:01:52\n",
      "SAR2: S1A_EW_GRDM_1SDH_20230109T074532_20230109T074637_046704_05992E_8779.SAFE \n",
      "timestamp: 2023-01-09 07:45:32\n",
      "Pair 5:\n",
      "SAR1: S1A_EW_GRDM_1SDH_20230119T080151_20230119T080256_046850_059E12_3404.SAFE \n",
      "timestamp: 2023-01-19 08:01:51\n",
      "SAR2: S1A_EW_GRDM_1SDH_20230121T074532_20230121T074636_046879_059F17_E3F6.SAFE \n",
      "timestamp: 2023-01-21 07:45:32\n",
      "Pair 6:\n",
      "SAR1: S1A_EW_GRDM_1SDH_20230210T081814_20230210T081918_047171_05A8EB_9B9D.SAFE \n",
      "timestamp: 2023-02-10 08:18:14\n",
      "SAR2: S1A_EW_GRDM_1SDH_20230212T080151_20230212T080255_047200_05A9D0_C7EB.SAFE \n",
      "timestamp: 2023-02-12 08:01:51\n",
      "Pair 7:\n",
      "SAR1: S1A_EW_GRDM_1SDH_20230212T080151_20230212T080255_047200_05A9D0_C7EB.SAFE \n",
      "timestamp: 2023-02-12 08:01:51\n",
      "SAR2: S1A_EW_GRDM_1SDH_20230214T074531_20230214T074635_047229_05AAC8_4014.SAFE \n",
      "timestamp: 2023-02-14 07:45:31\n",
      "Pair 8:\n",
      "SAR1: S1A_EW_GRDM_1SDH_20230320T080151_20230320T080255_047725_05BB9A_BFB2.SAFE \n",
      "timestamp: 2023-03-20 08:01:51\n",
      "SAR2: S1A_EW_GRDM_1SDH_20230322T074531_20230322T074635_047754_05BCA1_F9D9.SAFE \n",
      "timestamp: 2023-03-22 07:45:31\n",
      "Pair 9:\n",
      "SAR1: S1A_EW_GRDM_1SDH_20230327T075355_20230327T075459_047827_05BF0B_E30B.SAFE \n",
      "timestamp: 2023-03-27 07:53:55\n",
      "SAR2: S1A_EW_GRDM_1SDH_20230329T073729_20230329T073833_047856_05C011_8100.SAFE \n",
      "timestamp: 2023-03-29 07:37:29\n"
     ]
    }
   ],
   "source": [
    "# 1. Prepare SAR pairs\n",
    "\n",
    "# Collect Sentinel SAFE objects for files in safe directory.\n",
    "safe_objects = s1_preparation.collect_sentinel_files(safe_folder, path_to_HH_files, path_to_HV_files,  S1_safe_regex, S1_prod_regex)\n",
    "\n",
    "# Get pairs of Sentinel SAFE objects where their timestamps are within 50 hours of each other.\n",
    "sar_pairs = s1_preparation.get_pairs_within_time_limit(safe_objects, hours = 50)\n",
    "\n",
    "# Print details for each pair.\n",
    "for index, pair in enumerate(sar_pairs, start=1):  # start=1 makes the index start from 1\n",
    "    print(f'Pair {index}:')\n",
    "    print(f'SAR1: {pair[0].filename} \\ntimestamp: {pair[0].timestamp}\\n'\n",
    "          f'SAR2: {pair[1].filename} \\ntimestamp: {pair[1].timestamp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb6ccdd0-3680-47e3-8348-feb283662131",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sar_pairs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m dst_dom \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Initialize to None\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Loop over all pairs and use enumerate to get an index for each pair\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, pair \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43msar_pairs\u001b[49m, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):  \u001b[38;5;66;03m# start=1 to have human-friendly indexing\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Create a Profile object\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     pr \u001b[38;5;241m=\u001b[39m cProfile\u001b[38;5;241m.\u001b[39mProfile()\n\u001b[1;32m     19\u001b[0m     pr\u001b[38;5;241m.\u001b[39menable()  \u001b[38;5;66;03m# Start profiling\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sar_pairs' is not defined"
     ]
    }
   ],
   "source": [
    "# Define mod_dom and dst_dom outside the loop\n",
    "mod_res = 2500\n",
    "mod_dom = None  # Initialize to None\n",
    "dst_res = 100\n",
    "dst_dom = None  # Initialize to None\n",
    "\n",
    "\n",
    "\n",
    "# Loop over all pairs and use enumerate to get an index for each pair\n",
    "for index, pair in enumerate(sar_pairs, start=1):  # start=1 to have human-friendly indexing\n",
    "    \n",
    "    # Create a Profile object\n",
    "    pr = cProfile.Profile()\n",
    "    pr.enable()  # Start profiling\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 2.1. Prepare nansat objects and domains for HV polarisation\n",
    "    n1_hv, n2_hv, output_dir_name, plots_dir_hv = domains_preparation.prepare_nansat_objects(\n",
    "        pair[0], pair[1], output_folder, polarisation='HV')\n",
    "    \n",
    "    # Prepare nansat objects and domains for HH polarisation\n",
    "    n1_hh, n2_hh, output_dir_name, plots_dir_hh = domains_preparation.prepare_nansat_objects(\n",
    "        pair[0], pair[1], output_folder, polarisation='HH')\n",
    "    \n",
    "\n",
    "    # Additional processing steps\n",
    "    # 2.2  Define model domain (mod_dom) for comparing drift and comparison (dst_dom) domain to compare SAR images (real and forecasted)\n",
    "    \n",
    "    '''\n",
    "    # Pick this if want create mod and comparison domain for each pair (then they can differ)\n",
    "    # Prepare subset model grid for domains and pattern matching\n",
    "    X_subset, Y_subset, lon_subset, lat_subset = domains_preparation.prepare_grid(n1_hv, n2_hv, srs, X, Y, lon, lat, buffer=0)\n",
    "    \n",
    "    # Set a model domain\n",
    "    mod_res = 2500\n",
    "    mod_dom = Domain(srs, f'-te {min(X_subset.data)} {min(Y_subset.data) - mod_res * 2} {max(X_subset.data) + mod_res} {max(Y_subset.data)} -tr {mod_res} {mod_res}')\n",
    "    \n",
    "    \n",
    "    lon1pm, lat1pm = mod_dom.get_geolocation_grids()\n",
    "    x, y = mod_dom.get_geolocation_grids(dst_srs=srs)\n",
    "    \n",
    "    \n",
    "    # Set a comparison domain \n",
    "    dst_res = 100\n",
    "    dst_dom = Domain(srs, f'-te {min(X_subset.data)} {min(Y_subset.data) - dst_res * 2} {max(X_subset.data) + dst_res} {max(Y_subset.data)} -tr {dst_res} {dst_res}')\n",
    "    '''\n",
    "    \n",
    "    # Check if mod_dom and dst_dom are None, if so, define them based on the first pair\n",
    "    if mod_dom is None:\n",
    "        X_subset, Y_subset, lon_subset, lat_subset, min_row, max_row, min_col, max_col = domains_preparation.prepare_grid(n1_hv, n2_hv, srs, X, Y, lon, lat, buffer=0)\n",
    "        mod_dom = Domain(srs, f'-te {min(X_subset.data)} {min(Y_subset.data) - mod_res * 2} {max(X_subset.data) + mod_res} {max(Y_subset.data)} -tr {mod_res} {mod_res}')\n",
    "        lon1pm, lat1pm = mod_dom.get_geolocation_grids()\n",
    "        x, y = mod_dom.get_geolocation_grids(dst_srs=srs)\n",
    "    \n",
    "    if dst_dom is None:\n",
    "        dst_dom = Domain(srs, f'-te {min(X_subset.data)} {min(Y_subset.data) - dst_res * 2} {max(X_subset.data) + dst_res} {max(Y_subset.data)} -tr {dst_res} {dst_res}')\n",
    "        \n",
    "    \n",
    "    domains_preparation.plot_borders(mod_dom, n1_hv, n2_hv, output_dir_name) # borders for hh and hv are the same\n",
    "    # Checking that domains have the same borders\n",
    "    \n",
    "    rows1, cols1 = dst_dom.shape()\n",
    "    print(\"dst_dom corner coordinates:\", dst_dom.transform_points([0,cols1-1,0,cols1-1], [0,0,rows1-1,rows1-1], dst_srs=srs))\n",
    "    \n",
    "    rows1, cols1 = mod_dom.shape()\n",
    "    print(\"mod_dom corner coordinates:\", mod_dom.transform_points([0,cols1-1,0,cols1-1], [0,0,rows1-1,rows1-1], dst_srs=srs))\n",
    "    \n",
    "    # 3.   Retrieve reference drift\n",
    "    # 3.1. Run feature tracking and pattern matching for HV\n",
    "    \n",
    "   \n",
    "    # Run feature tracking and plot results \n",
    "    c1_hv, r1_hv, c2_hv, r2_hv = SAR1_SAR2_drift_retrivial.run_feature_tracking(n1_hv, n2_hv, plots_dir_hv)\n",
    "    \n",
    "    #Run pattern matching and plot results\n",
    "    upm_hv, vpm_hv, apm_hv, rpm_hv, hpm_hv, ssim_hv, lon2pm_hv, lat2pm_hv = SAR1_SAR2_drift_retrivial.run_pattern_matching(plots_dir_hv, x, y, \n",
    "                                                               lon1pm, lat1pm, n1_hv, c1_hv, r1_hv, n2_hv, c2_hv, r2_hv, srs, \n",
    "                                                               min_border=200,\n",
    "                                                               max_border=200,\n",
    "                                                               #min_border=10, #test\n",
    "                                                               #max_border=10, #test\n",
    "                                                               #angles=[-9,-6, -3, 0, 3, 6, 9]) #test\n",
    "                                                               angles=[-50, -45, -40, -35, -30, -25, -20, -15,-12, -9,-6, -3, 0, 3, 6, 9, 12,15, 20, 25, 30, 35, 40, 45, 50])\n",
    "    # 3.2. Run feature tracking and pattern matching for HH\n",
    "    \n",
    "    # HH Processing\n",
    "    # Run feature tracking and plot results \n",
    "    c1_hh, r1_hh, c2_hh, r2_hh = SAR1_SAR2_drift_retrivial.run_feature_tracking(n1_hh, n2_hh, plots_dir_hh)\n",
    "    \n",
    "    #Run pattern matching and plot results\n",
    "    upm_hh, vpm_hh, apm_hh, rpm_hh, hpm_hh, ssim_hh, lon2pm_hh, lat2pm_hh = SAR1_SAR2_drift_retrivial.run_pattern_matching(plots_dir_hh, x, y, \n",
    "                                                               lon1pm, lat1pm, n1_hh, c1_hh, r1_hh, n2_hh, c2_hh, r2_hh,srs, \n",
    "                                                               min_border=200,\n",
    "                                                               max_border=200,\n",
    "                                                               #min_border=10, #test\n",
    "                                                               #max_border=10, #test\n",
    "                                                               #angles=[-9,-6, -3, 0, 3, 6, 9]) #test\n",
    "                                                               angles=[-50, -40, -35, -30, -25, -20, -15,-12, -9,-6, -3, 0, 3, 6, 9, 12,15, 20, 25, 30, 35, 40, 50 ])\n",
    "    \n",
    "    \n",
    "    # 3.3. Get combined drift and all textural parameters\n",
    "    \n",
    "    # Combining hh and hv results based on hessian threshold\n",
    "    upm, vpm, apm, rpm, hpm, ssim, lon2pm, lat2pm = SAR1_SAR2_drift_retrivial.combine_hh_hv(output_dir_name, x, y, upm_hh, vpm_hh, apm_hh, rpm_hh, hpm_hh, ssim_hh, lon2pm_hh, lat2pm_hh,\n",
    "                                  upm_hv, vpm_hv, apm_hv, rpm_hv, hpm_hv, ssim_hv, lon2pm_hv, lat2pm_hv)\n",
    "    # 3.4.  Get good pixel indices based on hessian and neighbor thresholds.\n",
    "    \n",
    "    #Returns:\n",
    "    #    - gpi1: Good pixel index based on hessian value\n",
    "    #    - gpi2: Good pixel index combining hessian and neighbors count \n",
    "    \n",
    "    hessian=8\n",
    "    neighbors=2\n",
    "    \n",
    "    gpi1, gpi2 = SAR1_SAR2_drift_retrivial.get_good_pixel_indices(hpm, h_threshold=hessian, neighbors_threshold=neighbors)\n",
    "    \n",
    "        \n",
    "    # Plot the filtering results\n",
    "    general_plots_path = SAR1_SAR2_drift_retrivial.plot_filter_results(output_dir_name, x, y, hpm, upm, vpm, gpi1, gpi2, hessian, neighbors)\n",
    "    \n",
    "    \n",
    "    #  Save final drift, its parameters and filtering arrays to npy files\n",
    "    save_name = 'sar_drift_output'\n",
    "    sar_drift_output_path = SAR1_SAR2_drift_retrivial.save_sar_drift_results(output_dir_name, save_name,\n",
    "                                                                             upm=upm, vpm=vpm, apm=apm, rpm=rpm, \n",
    "                                                                             hpm=hpm, ssim=ssim, lon2pm=lon2pm, \n",
    "                                                                             lat2pm=lat2pm, gpi1=gpi1, gpi2=gpi2)\n",
    "    # 4. Warp SAR1 image with the reference sar drift and compare all arrays in the comparison distination domain\n",
    "    \n",
    "    # 4.1. Warp\n",
    "    # Warp SAR1 with SAR-drift compenstaion/displacement\n",
    "    good_pixels = gpi2\n",
    "    mask_pm = ~good_pixels # mask out low quality or NaN\n",
    "    s1_dst_dom_S_hv = warping_with_domain.warp_with_uv(n1_hv, n1_hv[1], mod_dom, upm, vpm, mask_pm, dst_dom)\n",
    "    s1_dst_dom_S_hh = warping_with_domain.warp_with_uv(n1_hh, n1_hh[1], mod_dom, upm, vpm, mask_pm, dst_dom)\n",
    "    \n",
    "    # Warp SAR2 to the comparison domain\n",
    "    s2_dst_dom_hv = warping_with_domain.warp(n2_hv, n2_hv[1], dst_dom)\n",
    "    s2_dst_dom_hh = warping_with_domain.warp(n2_hh, n2_hh[1], dst_dom)\n",
    "    \n",
    "    # Warp SAR1 to the comparison domain for visualisation\n",
    "    s1_dst_dom_hv = warping_with_domain.warp(n1_hv, n1_hv[1], dst_dom)\n",
    "    s1_dst_dom_hh = warping_with_domain.warp(n1_hh, n1_hh[1], dst_dom)\n",
    "    # 4.2. Plot warping results\n",
    "    warping_save_dir = os.path.join(output_dir_name, \"Warping_plots\")\n",
    "    os.makedirs(warping_save_dir, exist_ok=True)\n",
    "    warping_with_domain.plot_sar_forecast_images(warping_save_dir, \n",
    "                                             \"Forecast_with_sar_ref_drift\", \n",
    "                                             s1_dst_dom_hv, s2_dst_dom_hv, s1_dst_dom_S_hv,\n",
    "                                             s1_dst_dom_hh, s2_dst_dom_hh, s1_dst_dom_S_hh,\n",
    "                                             gamma_value=1.2)\n",
    "    # 5. Calculate quality parametrs (corr, hess, ssim) for the predicted SAR2 (by calculating pattern matchin on SAR2 and SAR2_predicted)\n",
    "    \n",
    "    # 5.1. Make new nansat objects for comparison\n",
    "    \n",
    "    n_s1_predict = Nansat.from_domain(dst_dom, array = s1_dst_dom_S_hv)\n",
    "    n_s2 = Nansat.from_domain(dst_dom, array = s2_dst_dom_hv)\n",
    "    \n",
    "    # 5.2. Create directory for saving plots \n",
    "    comparison_dir = os.path.join(output_dir_name, f\"SAR_distort_error_plots\")\n",
    "    try:\n",
    "        os.makedirs(comparison_dir, exist_ok=True)\n",
    "        print(f\"Successfully created {comparison_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create {comparison_dir}. Error: {e}\")\n",
    "        \n",
    "    # Calculate realibility indexes \n",
    "    \n",
    "    \n",
    "    # 5.4. Run feature tracking and plot results \n",
    "    c1_alg_hv, r1_alg_hv, c2_alg_hv, r2_alg_hv = SAR1_SAR2_drift_retrivial.run_feature_tracking(n_s1_predict, n_s2, comparison_dir)\n",
    "    \n",
    "    # 5.5. Run pattern matching and plot results\n",
    "    upm_alg_hv, vpm_alg_hv, apm_alg_hv, rpm_alg_hv, hpm_alg_hv, ssim_alg_hv, lon2pm_alg_hv, lat2pm_alg_hv = SAR1_SAR2_drift_retrivial.run_pattern_matching(comparison_dir, x, y, \n",
    "                                                               lon1pm, lat1pm, n_s1_predict, c1_alg_hv, r1_alg_hv, n_s2, c2_alg_hv, r2_alg_hv, srs, \n",
    "                                                               min_border=200,\n",
    "                                                               max_border=200,\n",
    "                                                               #min_border=10, #test\n",
    "                                                               #max_border=10, #test\n",
    "                                                               angles=[-15,-12,-9,-6, -3, 0, 3, 6, 9, 12, 15]) #light\n",
    "                                                               #angles=[-50, -45, -40, -35, -30, -25, -20, -15,-12, -9,-6, -3, 0, 3, 6, 9, 12,15, 20, 25, 30, 35, 40, 45, 50])\n",
    "    \n",
    "    # 5.6. Save comparison results, its parameters and filtering arrays to npy files\n",
    "    save_name = 'sar_distort_error_data'\n",
    "    sar_drift_output_path = SAR1_SAR2_drift_retrivial.save_sar_drift_results(comparison_dir, save_name,\n",
    "                                                                             upm=upm, vpm=vpm, apm=apm, rpm=rpm, \n",
    "                                                                             hpm=hpm, ssim=ssim, lon2pm=lon2pm, \n",
    "                                                                             lat2pm=lat2pm, gpi1=gpi1, gpi2=gpi2)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Pair {index} processed in {end_time - start_time:.2f} seconds.\")\n",
    "    \n",
    "    pr.disable()  # Stop profiling\n",
    "    s = StringIO()\n",
    "    ps = pstats.Stats(pr, stream=s).sort_stats('cumulative')\n",
    "    ps.print_stats()\n",
    "\n",
    "    # Get the profiling results as a string and print it\n",
    "    profiling_results = s.getvalue()\n",
    "    # Save the stats to a file\n",
    "    profiling_dir_path = os.path.join(output_dir_name, \"profiling\")\n",
    "    os.makedirs(profiling_dir_path, exist_ok=True)\n",
    "    save_path = os.path.join(profiling_dir_path, f\"profile_results_pair{index}.prof\")\n",
    "    print(f'profiling path is {save_path}')\n",
    "    ps.dump_stats(save_path)\n",
    "    gc.collect()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d731e20f-e5d6-4752-8e9c-d5c10d44ce3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7dc726-d05c-43e5-b91b-fbada9c658bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
